# üëÅÔ∏è Vision Module Setup Guide

## üéØ What You Get
- **Ask "What do you see?"** ‚Üí Jarvis analyzes your screen
- **"Describe this image"** ‚Üí Analyzes any image file
- **Free & Fast** ‚Üí MiniCPM-V model on Colab GPU (2-3s per analysis)
- **Safe & Isolated** ‚Üí Separate files, won't break main Jarvis

---

## ‚ö° Quick Setup (5 minutes)

### **Step 1: Add to Colab Notebook**

Open `jarvis_colab_python312.ipynb` and add these cells at the end:

1. Copy **Cell 10-13** from `VISION_COLAB_CELLS.py`
2. Paste them after your existing Cell 9 (ngrok setup)
3. Run Cell 10 ‚Üí Installs vision dependencies (~1 min)
4. Run Cell 11 ‚Üí Loads MiniCPM-V model (~3-5 min first time)
5. Run Cell 12 ‚Üí Adds vision endpoint to Flask
6. Run Cell 13 ‚Üí Tests the vision system

### **Step 2: Get Vision URL**

The vision endpoint shares the same Flask server as TTS!

```python
# Your ngrok URL from Cell 9 is ALREADY the vision URL
# Example: https://7b86d3ce40d7.ngrok-free.app
```

### **Step 3: Set Environment Variable**

```powershell
$env:VISION_URL="https://YOUR-TTS-URL.ngrok-free.app"
# Same URL as TTS_URL!
```

### **Step 4: Install Dependencies**

```powershell
pip install Pillow
```

### **Step 5: Test It!**

```powershell
python test_vision.py
```

---

## üéÆ Usage Examples

### **Standalone Test:**
```powershell
python test_vision.py
```

Interactive menu to:
1. Analyze your current screen
2. Ask custom questions
3. Analyze image files

### **In Main Jarvis (Optional Integration):**

To add vision to main Jarvis voice commands, edit `main_text_elevenlabs.py`:

```python
# Add at top
from tools.vision import analyze_screen

# In conversation loop, add voice commands like:
if "what do you see" in user_input.lower():
    result = analyze_screen.invoke({"question": user_input})
    speak_text(result)
```

---

## üìä Performance

| Task | Speed | Model |
|------|-------|-------|
| Screen capture | <0.5s | Native |
| AI vision analysis | 2-3s | MiniCPM-V (GPU) |
| **Total response** | **~3s** | ‚ö° Fast! |

---

## üéØ What It Can Do

‚úÖ **Describe what's on screen**
- "What do you see?"
- "What app is open?"
- "Read the text on my screen"

‚úÖ **Analyze images**
- "What's in this photo?"
- "Describe this screenshot"
- "What does this diagram show?"

‚úÖ **Answer questions about visuals**
- "What color is the button?"
- "How many windows are open?"
- "What's the title of this page?"

---

## üîß Troubleshooting

**Vision URL not set:**
- Use your TTS_URL! Both endpoints share the same Flask server

**"Vision system unavailable":**
- Make sure Colab Cells 10-13 are running
- Check that Cell 12 added the `/vision` endpoint
- Test: `curl YOUR-URL/health` should return 200

**Slow first response:**
- First vision call loads the model (~5-10s)
- Subsequent calls are fast (2-3s)

**Import error:**
- Run: `pip install Pillow`

---

## üöÄ Advanced: Voice Commands

Add these patterns to recognize vision requests:

```python
vision_triggers = [
    "what do you see",
    "describe the screen",
    "what's on my screen",
    "analyze this",
    "look at this"
]

if any(trigger in user_input.lower() for trigger in vision_triggers):
    # Use vision tool
    result = analyze_screen.invoke({"question": user_input})
    speak_text(result)
```

---

## üí° Pro Tips

1. **Screen analysis is instant** - No need to save screenshots first
2. **Works with multiple monitors** - Captures all screens
3. **Questions matter** - Be specific: "What's the error message?" vs "What do you see?"
4. **Model is smart** - Can read text, identify apps, describe layouts

---

## üìÅ Files Created

```
jarvis/
‚îú‚îÄ‚îÄ vision_remote.py          # Vision client (connects to Colab)
‚îú‚îÄ‚îÄ tools/vision.py            # LangChain tool wrapper
‚îú‚îÄ‚îÄ test_vision.py             # Standalone test script
‚îú‚îÄ‚îÄ VISION_COLAB_CELLS.py      # Colab cells to add
‚îî‚îÄ‚îÄ VISION_SETUP.md            # This guide
```

**Safe & Isolated** - If vision breaks, main Jarvis still works! ‚úÖ

---

**Ready to see the world! üëÅÔ∏è**
