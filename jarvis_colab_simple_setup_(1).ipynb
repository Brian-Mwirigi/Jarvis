{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7ff37e7e",
      "metadata": {
        "id": "7ff37e7e"
      },
      "source": [
        "# Jarvis \u00e2\u20ac\u201d Simple Colab Setup (Vision + Ollama proxy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb9b440",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fb9b440",
        "outputId": "b8214fb2-ada8-4703-d899-25c7b0d322b1"
      },
      "outputs": [],
      "source": [
        "# Cell 1 \u00e2\u20ac\u201d Check GPU and device\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "791ae058",
      "metadata": {
        "id": "791ae058"
      },
      "outputs": [],
      "source": [
        "# Cell 2 \u00e2\u20ac\u201d Install minimal dependencies\n",
        "!pip install -q flask pyngrok requests transformers pillow accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c06ce2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0c06ce2",
        "outputId": "3cf4b1b1-469f-4003-e4f5-233628416909"
      },
      "outputs": [],
      "source": [
        "# Cell 3 \u00e2\u20ac\u201d Install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh || true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ac98445",
      "metadata": {
        "id": "0ac98445"
      },
      "outputs": [],
      "source": [
        "# Cell 4 \u00e2\u20ac\u201d Start Ollama server and pull model\n",
        "import subprocess, time, os\n",
        "print('Starting ollama serve...')\n",
        "try:\n",
        "    ollama_proc = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    time.sleep(3)\n",
        "    print('Ollama started (pid:', ollama_proc.pid, ')')\n",
        "except Exception as e:\n",
        "    print('Could not start ollama serve:', e)\n",
        "\n",
        "print('Pulling phi model (this can take a couple minutes)...')\n",
        "try:\n",
        "    subprocess.run(['ollama', 'pull', 'phi'], check=True)\n",
        "    print('Pulled phi model')\n",
        "except Exception as e:\n",
        "    print('Warning: ollama pull phi failed or skipped:', e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197a4cd6",
      "metadata": {
        "id": "197a4cd6"
      },
      "outputs": [],
      "source": [
        "# Cell 5 \u00e2\u20ac\u201d Load vision model and create Flask app with /vision and /proxy_ollama\n",
        "from flask import Flask, request, jsonify\n",
        "import base64, io, gc, torch, time\n",
        "from PIL import Image\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "import requests\n",
        "\n",
        "app = Flask(__name__)\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({'status': 'healthy'})\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Loading BLIP-2 model on', device)\n",
        "processor = Blip2Processor.from_pretrained('Salesforce/blip2-opt-2.7b')\n",
        "model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b', torch_dtype=torch.float16, device_map='auto')\n",
        "print('BLIP-2 loaded')\n",
        "\n",
        "@app.route('/vision', methods=['POST'])\n",
        "def vision_endpoint():\n",
        "    try:\n",
        "        data = request.get_json(force=True)\n",
        "        image_b64 = data.get('image','')\n",
        "        question = data.get('question','What do you see in this image?')\n",
        "        if not image_b64:\n",
        "            return jsonify({'error':'no image'}), 400\n",
        "        image = Image.open(io.BytesIO(base64.b64decode(image_b64))).convert('RGB')\n",
        "        prompt = f'Question: {question} Answer:'\n",
        "        inputs = processor(images=image, text=prompt, return_tensors='pt').to(device, torch.float16)\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n",
        "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "        if 'Answer:' in generated_text:\n",
        "            answer = generated_text.split('Answer:')[-1].strip()\n",
        "        else:\n",
        "            answer = generated_text\n",
        "        return jsonify({'answer': answer, 'question': question})\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "@app.route('/proxy_ollama', methods=['POST'])\n",
        "def proxy_ollama():\n",
        "    try:\n",
        "        raw_body = request.get_data()\n",
        "        forward_headers = {k: v for k, v in request.headers.items() if k.lower() in ('content-type','user-agent','accept','ngrok-skip-browser-warning')}\n",
        "        # Increased timeout to 120s for larger models like llama3.1:8b\n",
        "        resp = requests.post('http://localhost:11434/api/chat', data=raw_body, headers=forward_headers, timeout=120)\n",
        "        try:\n",
        "            return jsonify(resp.json()), resp.status_code\n",
        "        except Exception:\n",
        "            return (resp.text, resp.status_code, {'Content-Type': resp.headers.get('Content-Type','text/plain')})\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "print('Flask app created with /health, /vision, /proxy_ollama')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EWBEEStCgBCs",
      "metadata": {
        "id": "EWBEEStCgBCs"
      },
      "outputs": [],
      "source": [
        "#Ummmm idk what to call this\n",
        "from getpass import getpass\n",
        "from pyngrok import ngrok\n",
        "\n",
        "token = getpass(\"Enter your ngrok authtoken (): \")\n",
        "ngrok.set_auth_token(token)\n",
        "print(\"ngrok authtoken set \u00e2\u20ac\u201d you can now create tunnels.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d35de2f",
      "metadata": {
        "id": "8d35de2f"
      },
      "outputs": [],
      "source": [
        "# Cell 6 \u00e2\u20ac\u201d Start Flask and ngrok tunnels\n",
        "from pyngrok import ngrok\n",
        "from threading import Thread\n",
        "import time\n",
        "\n",
        "print('Creating ngrok tunnels...')\n",
        "ollama_tunnel = ngrok.connect(11434, bind_tls=True)\n",
        "flask_tunnel = ngrok.connect(5000, bind_tls=True)\n",
        "flask_url = flask_tunnel.public_url.replace('http://','https://')\n",
        "ollama_url = ollama_tunnel.public_url.replace('http://','https://')\n",
        "print('Flask URL:', flask_url)\n",
        "print('Ollama URL:', ollama_url)\n",
        "\n",
        "def run_flask():\n",
        "    app.run(host='0.0.0.0', port=5000, use_reloader=False)\n",
        "\n",
        "thread = Thread(target=run_flask, daemon=True)\n",
        "thread.start()\n",
        "time.sleep(3)\n",
        "print('Flask server started (background thread)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afcc7179",
      "metadata": {
        "id": "afcc7179"
      },
      "outputs": [],
      "source": [
        "# Cell 7 \u00e2\u20ac\u201d Basic tests: Vision and Proxy\n",
        "import io, base64, requests, json\n",
        "from PIL import Image\n",
        "\n",
        "# Create a tiny red image\n",
        "img = Image.new('RGB', (100,100), color='red')\n",
        "buf = io.BytesIO()\n",
        "img.save(buf, format='PNG')\n",
        "b64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "\n",
        "# Local vision test (increased timeout for first inference warmup)\n",
        "print(\"Testing vision (first call may take 30-60s for model warmup)...\")\n",
        "try:\n",
        "    r = requests.post('http://localhost:5000/vision', json={'image': b64, 'question': 'What color is this?'}, timeout=120)\n",
        "    print('Local vision status:', r.status_code, r.json())\n",
        "except Exception as e:\n",
        "    print('Local vision test failed:', e)\n",
        "\n",
        "# Proxy test via public flask URL\n",
        "print(\"\\nTesting Ollama proxy...\")\n",
        "try:\n",
        "    payload = {\n",
        "        'model': 'phi',\n",
        "        'messages': [\n",
        "            {'role': 'system', 'content': 'You are Jarvis. Answer in one short sentence.'},\n",
        "            {'role': 'user', 'content': 'Say hello.'}\n",
        "        ]\n",
        "    }\n",
        "    resp = requests.post(flask_url + '/proxy_ollama', json=payload, timeout=60)\n",
        "    print('Proxy status:', resp.status_code)\n",
        "    try:\n",
        "        print('Proxy JSON:', resp.json())\n",
        "    except Exception:\n",
        "        print('Proxy text:', resp.text[:200])\n",
        "except Exception as e:\n",
        "    print('Proxy test failed:', e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c42d859e",
      "metadata": {
        "id": "c42d859e"
      },
      "outputs": [],
      "source": [
        "# Cell 8 \u00e2\u20ac\u201d Keep-alive loop\n",
        "import time, requests\n",
        "print('Starting keep-alive loop \u00e2\u20ac\u201d stop manually to end')\n",
        "while True:\n",
        "    try:\n",
        "        requests.get('http://localhost:5000/health', timeout=5)\n",
        "        print('heartbeat...', end='\\r')\n",
        "    except Exception:\n",
        "        pass\n",
        "    time.sleep(60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}