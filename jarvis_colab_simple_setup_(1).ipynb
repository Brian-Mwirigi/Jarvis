{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7ff37e7e",
      "metadata": {
        "id": "7ff37e7e"
      },
      "source": [
        "# Jarvis — Simple Colab Setup (Vision + Ollama proxy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb9b440",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fb9b440",
        "outputId": "b8214fb2-ada8-4703-d899-25c7b0d322b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Cell 1 — Check GPU and device\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "791ae058",
      "metadata": {
        "id": "791ae058"
      },
      "outputs": [],
      "source": [
        "# Cell 2 — Install minimal dependencies\n",
        "!pip install -q flask pyngrok requests transformers pillow accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c06ce2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0c06ce2",
        "outputId": "3cf4b1b1-469f-4003-e4f5-233628416909"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "# Cell 3 — Install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh || true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ac98445",
      "metadata": {
        "id": "0ac98445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting ollama serve...\n",
            "Ollama started (pid: 883 )\n",
            "Pulling phi model (this can take a couple minutes)...\n",
            "Pulled phi model\n"
          ]
        }
      ],
      "source": [
        "# Cell 4 — Start Ollama server and pull model\n",
        "import subprocess, time, os\n",
        "print('Starting ollama serve...')\n",
        "try:\n",
        "    ollama_proc = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    time.sleep(3)\n",
        "    print('Ollama started (pid:', ollama_proc.pid, ')')\n",
        "except Exception as e:\n",
        "    print('Could not start ollama serve:', e)\n",
        "\n",
        "print('Pulling phi model (this can take a couple minutes)...')\n",
        "try:\n",
        "    subprocess.run(['ollama', 'pull', 'phi'], check=True)\n",
        "    print('Pulled phi model')\n",
        "except Exception as e:\n",
        "    print('Warning: ollama pull phi failed or skipped:', e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197a4cd6",
      "metadata": {
        "id": "197a4cd6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading BLIP-2 model on cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bf5da14084f4f9fb12c29e1c30a0275",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e49bd8fbf25d4e51b48c7e55552e357c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "processor_config.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3dab5af96d884845b1abc12ef2990485",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/882 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ddcfa57c79c46e0ac414618774adff3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "625ce4f08a5f4e9db57af6d51c04a695",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92c999b0622f44e9a3c2dcf1487a7ef6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05c53bbe6501437386586371413ac7d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c49b1c9cdc2443d6ac18711dc19308e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/548 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b5ff793f6b744caaf9cf8d8ee932480",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a5286422a89477b9b05fcc1c489e28a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "425fc30172274394ba583e4b93631b43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03dc03eee7c646929f4ae323af35fd1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ca02109cc734844b8df140f1819d296",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2cf80a9066994b06a720de7ebc8d3a8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "316c3f8146934bdeb434e3d75c5d1548",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLIP-2 loaded\n",
            "Flask app created with /health, /vision, /proxy_ollama\n"
          ]
        }
      ],
      "source": [
        "# Cell 5 — Load vision model and create Flask app with /vision and /proxy_ollama\n",
        "from flask import Flask, request, jsonify\n",
        "import base64, io, gc, torch, time\n",
        "from PIL import Image\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "import requests\n",
        "\n",
        "app = Flask(__name__)\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({'status': 'healthy'})\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Loading BLIP-2 model on', device)\n",
        "processor = Blip2Processor.from_pretrained('Salesforce/blip2-opt-2.7b')\n",
        "model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b', torch_dtype=torch.float16, device_map='auto')\n",
        "print('BLIP-2 loaded')\n",
        "\n",
        "@app.route('/vision', methods=['POST'])\n",
        "def vision_endpoint():\n",
        "    try:\n",
        "        data = request.get_json(force=True)\n",
        "        image_b64 = data.get('image','')\n",
        "        question = data.get('question','What do you see in this image?')\n",
        "        if not image_b64:\n",
        "            return jsonify({'error':'no image'}), 400\n",
        "        image = Image.open(io.BytesIO(base64.b64decode(image_b64))).convert('RGB')\n",
        "        prompt = f'Question: {question} Answer:'\n",
        "        inputs = processor(images=image, text=prompt, return_tensors='pt').to(device, torch.float16)\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n",
        "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "        if 'Answer:' in generated_text:\n",
        "            answer = generated_text.split('Answer:')[-1].strip()\n",
        "        else:\n",
        "            answer = generated_text\n",
        "        return jsonify({'answer': answer, 'question': question})\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "@app.route('/proxy_ollama', methods=['POST'])\n",
        "def proxy_ollama():\n",
        "    try:\n",
        "        raw_body = request.get_data()\n",
        "        forward_headers = {k: v for k, v in request.headers.items() if k.lower() in ('content-type','user-agent','accept','ngrok-skip-browser-warning')}\n",
        "        # Increased timeout to 120s for larger models like llama3.1:8b\n",
        "        resp = requests.post('http://localhost:11434/api/chat', data=raw_body, headers=forward_headers, timeout=120)\n",
        "        try:\n",
        "            return jsonify(resp.json()), resp.status_code\n",
        "        except Exception:\n",
        "            return (resp.text, resp.status_code, {'Content-Type': resp.headers.get('Content-Type','text/plain')})\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "print('Flask app created with /health, /vision, /proxy_ollama')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EWBEEStCgBCs",
      "metadata": {
        "id": "EWBEEStCgBCs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ngrok authtoken set — you can now create tunnels.                                                   \n"
          ]
        }
      ],
      "source": [
        "#Ummmm idk what to call this\n",
        "from getpass import getpass\n",
        "from pyngrok import ngrok\n",
        "\n",
        "token = getpass(\"Enter your ngrok authtoken (): \")\n",
        "ngrok.set_auth_token(token)\n",
        "print(\"ngrok authtoken set — you can now create tunnels.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d35de2f",
      "metadata": {
        "id": "8d35de2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating ngrok tunnels...\n",
            "Flask URL: https://398bd9768adb.ngrok-free.app\n",
            "Ollama URL: https://e5812955d885.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Flask server started (background thread)\n"
          ]
        }
      ],
      "source": [
        "# Cell 6 — Start Flask and ngrok tunnels\n",
        "from pyngrok import ngrok\n",
        "from threading import Thread\n",
        "import time\n",
        "\n",
        "print('Creating ngrok tunnels...')\n",
        "ollama_tunnel = ngrok.connect(11434, bind_tls=True)\n",
        "flask_tunnel = ngrok.connect(5000, bind_tls=True)\n",
        "flask_url = flask_tunnel.public_url.replace('http://','https://')\n",
        "ollama_url = ollama_tunnel.public_url.replace('http://','https://')\n",
        "print('Flask URL:', flask_url)\n",
        "print('Ollama URL:', ollama_url)\n",
        "\n",
        "def run_flask():\n",
        "    app.run(host='0.0.0.0', port=5000, use_reloader=False)\n",
        "\n",
        "thread = Thread(target=run_flask, daemon=True)\n",
        "thread.start()\n",
        "time.sleep(3)\n",
        "print('Flask server started (background thread)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afcc7179",
      "metadata": {
        "id": "afcc7179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Local vision test failed: HTTPConnectionPool(host='localhost', port=5000): Read timed out. (read timeout=30)\n",
            "Proxy test failed: HTTPSConnectionPool(host='398bd9768adb.ngrok-free.app', port=443): Read timed out. (read timeout=30)\n"
          ]
        }
      ],
      "source": [
        "# Cell 7 — Basic tests: Vision and Proxy\n",
        "import io, base64, requests, json\n",
        "from PIL import Image\n",
        "\n",
        "# Create a tiny red image\n",
        "img = Image.new('RGB', (100,100), color='red')\n",
        "buf = io.BytesIO()\n",
        "img.save(buf, format='PNG')\n",
        "b64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "\n",
        "# Local vision test (increased timeout for first inference warmup)\n",
        "print(\"Testing vision (first call may take 30-60s for model warmup)...\")\n",
        "try:\n",
        "    r = requests.post('http://localhost:5000/vision', json={'image': b64, 'question': 'What color is this?'}, timeout=120)\n",
        "    print('Local vision status:', r.status_code, r.json())\n",
        "except Exception as e:\n",
        "    print('Local vision test failed:', e)\n",
        "\n",
        "# Proxy test via public flask URL\n",
        "print(\"\\nTesting Ollama proxy...\")\n",
        "try:\n",
        "    payload = {\n",
        "        'model': 'phi',\n",
        "        'messages': [\n",
        "            {'role': 'system', 'content': 'You are Jarvis. Answer in one short sentence.'},\n",
        "            {'role': 'user', 'content': 'Say hello.'}\n",
        "        ]\n",
        "    }\n",
        "    resp = requests.post(flask_url + '/proxy_ollama', json=payload, timeout=60)\n",
        "    print('Proxy status:', resp.status_code)\n",
        "    try:\n",
        "        print('Proxy JSON:', resp.json())\n",
        "    except Exception:\n",
        "        print('Proxy text:', resp.text[:200])\n",
        "except Exception as e:\n",
        "    print('Proxy test failed:', e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c42d859e",
      "metadata": {
        "id": "c42d859e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/Dec/2025 15:50:38] \"GET /health HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Dec/2025 15:50:38] \"POST /proxy_ollama HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting keep-alive loop — stop manually to end\n",
            "heartbeat...\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/Dec/2025 15:51:03] \"POST /vision HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Dec/2025 15:51:38] \"GET /health HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Dec/2025 15:51:38] \"GET /health HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "heartbeat...\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/Dec/2025 15:52:38] \"GET /health HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "heartbeat...\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/Dec/2025 15:53:38] \"GET /health HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "heartbeat...\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/Dec/2025 15:54:38] \"GET /health HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "heartbeat...\r"
          ]
        }
      ],
      "source": [
        "# Cell 8 — Keep-alive loop\n",
        "import time, requests\n",
        "print('Starting keep-alive loop — stop manually to end')\n",
        "while True:\n",
        "    try:\n",
        "        requests.get('http://localhost:5000/health', timeout=5)\n",
        "        print('heartbeat...', end='\\r')\n",
        "    except Exception:\n",
        "        pass\n",
        "    time.sleep(60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
